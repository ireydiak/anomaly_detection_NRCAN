{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "# Normal label\n",
    "normal_label = \"Benign\"\n",
    "# Base path where to find the dataset\n",
    "base_path = \"../data/IDS2018/original\"\n",
    "# Base path where processed dataset will be stored\n",
    "export_path = \"../data/IDS2018\"\n",
    "# Name of the file summarizing the preprocessing\n",
    "info_fname = \"ids2018_info.csv\"\n",
    "# File name of the cleaned/processed dataset\n",
    "export_fname = \"ids2018.csv\"\n",
    "# Used to track preprocessing steps\n",
    "stats = defaultdict()\n",
    "stats[\"n_dropped_cols\"] = 0\n",
    "stats[\"n_dropped_rows\"] = 0\n",
    "# Columns to drop before any analysis\n",
    "cols_to_drop = [\n",
    "    'Flow ID',\n",
    "    'Src IP',\n",
    "    'Dst IP',\n",
    "    'Src Port',\n",
    "    'Dst Port'\n",
    "    'Protocol',\n",
    "    'Timestamp',\n",
    "]\n",
    "num_cols = [\n",
    "    'Tot Fwd Pkts',\n",
    "    'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts',\n",
    "    'TotLen Bwd Pkts',\n",
    "    'Fwd Pkt Len Max',\n",
    "    'Fwd Pkt Len Min',\n",
    "    'Fwd Pkt Len Mean',\n",
    "    'Fwd Pkt Len Std',\n",
    "    'Bwd Pkt Len Max',\n",
    "    'Bwd Pkt Len Min',\n",
    "    'Bwd Pkt Len Mean',\n",
    "    'Bwd Pkt Len Std',\n",
    "    'Flow Byts/s',\n",
    "    'Flow Pkts/s',\n",
    "    'Flow IAT Mean',\n",
    "    'Flow IAT Std',\n",
    "    'Flow IAT Max',\n",
    "    'Flow IAT Min',\n",
    "    'Fwd IAT Tot',\n",
    "    'Fwd IAT Mean',\n",
    "    'Fwd IAT Std',\n",
    "    'Fwd IAT Max',\n",
    "    'Fwd IAT Min',\n",
    "    'Bwd IAT Tot',\n",
    "    'Bwd IAT Mean',\n",
    "    'Bwd IAT Std',\n",
    "    'Bwd IAT Max',\n",
    "    'Bwd IAT Min',\n",
    "    'Fwd PSH Flags',\n",
    "    'Fwd URG Flags',\n",
    "    'Fwd Header Len',\n",
    "    'Bwd Header Len',\n",
    "    'Fwd Pkts/s',\n",
    "    'Bwd Pkts/s',\n",
    "    'Pkt Len Min',\n",
    "    'Pkt Len Max',\n",
    "    'Pkt Len Mean',\n",
    "    'Pkt Len Std',\n",
    "    'Pkt Len Var',\n",
    "    'FIN Flag Cnt',\n",
    "    'SYN Flag Cnt',\n",
    "    'RST Flag Cnt',\n",
    "    'PSH Flag Cnt',\n",
    "    'ACK Flag Cnt',\n",
    "    'URG Flag Cnt',\n",
    "    'CWE Flag Count',\n",
    "    'ECE Flag Cnt',\n",
    "    'Down/Up Ratio',\n",
    "    'Pkt Size Avg',\n",
    "    'Fwd Seg Size Avg',\n",
    "    'Bwd Seg Size Avg',\n",
    "    'Subflow Fwd Pkts',\n",
    "    'Subflow Fwd Byts',\n",
    "    'Subflow Bwd Pkts',\n",
    "    'Subflow Bwd Byts',\n",
    "    'Fwd Act Data Pkts',\n",
    "    'Fwd Seg Size Min',\n",
    "    'Active Mean',\n",
    "    'Active Std',\n",
    "    'Active Max',\n",
    "    'Active Min',\n",
    "    'Idle Mean',\n",
    "    'Idle Std',\n",
    "    'Idle Max',\n",
    "    'Idle Min'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in os.listdir(base_path):\n",
    "    chunk = pd.read_csv(os.path.join(base_path, f))\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "    chunk.loc[:, chunk.columns != \"Label\"] = chunk.loc[:, chunk.columns != \"Label\"].apply(pd.to_numeric, errors=\"coerce\")   \n",
    "    chunk.drop(cols_to_drop, axis=1, errors=\"ignore\")\n",
    "    df = pd.concat((df, chunk))\n",
    "    print(f)\n",
    "print(stats)\n",
    "df.to_csv(export_path + \"/ids2018_merged.csv\", index=False)\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop([\"Flow ID\", \"Src IP\", \"Src Port\", \"Dst IP\", \"Dst Port\", \"Protocol\", \"Timestamp\"], axis=1)\n",
    "df.to_csv(export_path + \"/ids2018_merged.csv\", index=False)\n",
    "df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(export_path + \"/ids2018_merged.csv\")\n",
    "stats[\"dropped_cols\"] = \"\"\n",
    "stats[\"n_dropped_cols\"] = 0\n",
    "stats[\"n_dropped_rows\"] = 0\n",
    "stats[\"n_instances\"] = len(df)\n",
    "stats[\"n_features\"] = df.shape[1] - 1\n",
    "stats[\"anomaly_ratio\"] = \"{:2.4f}\".format((df[\"Label\"] != normal_label).sum() / len(df))\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inner class imbalance\n",
    "Between anomalies, there is a strong class imbalance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Original class imbalance within attacks themselves\n",
    "mask = df[\"Label\"] != normal_label\n",
    "original_ad_ratios = pd.DataFrame(\n",
    "    pd.concat(\n",
    "        (df[mask][\"Label\"].value_counts(),\n",
    "        df[mask][\"Label\"].value_counts() / len(df[mask])), axis=1),\n",
    ")\n",
    "original_ad_ratios.to_csv(export_path + \"/ids2018_anomaly_labels_ratio.csv\")\n",
    "original_ad_ratios.columns = [\"Count\", \"Ratio\"]\n",
    "original_ad_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group DoS attacks\n",
    "mask = df[\"Label\"].str.startswith(\"DoS\")\n",
    "df.loc[mask, \"Label\"] = \"DoS\"\n",
    "\n",
    "# Group DDoS attacks\n",
    "mask = df[\"Label\"].str.startswith(\"DDoS\")\n",
    "df.loc[mask, \"Label\"] = \"DDoS\"\n",
    "mask = df[\"Label\"].str.startswith(\"DDOS\")\n",
    "df.loc[mask, \"Label\"] = \"DDoS\"\n",
    "\n",
    "# Group Web attacks\n",
    "mask = df[\"Label\"].str.startswith(\"Brute Force\")\n",
    "df.loc[mask, \"Label\"] = \"Web Attack\"\n",
    "mask = df[\"Label\"].str.startswith(\"SQL\")\n",
    "df.loc[mask, \"Label\"] = \"Web Attack\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Found 49 rows that are duplicates of the header row"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"Label\"] == \"Label\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updated class imbalance\n",
    "mask = df[\"Label\"] != normal_label\n",
    "mod_ad_ratios = pd.DataFrame(\n",
    "    pd.concat(\n",
    "        (df[mask][\"Label\"].value_counts(),\n",
    "        df[mask][\"Label\"].value_counts() / len(df[mask])), axis=1),\n",
    ")\n",
    "mod_ad_ratios.columns = [\"Count\", \"Ratio\"]\n",
    "mod_ad_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group DoS attacks\n",
    "mask = df[\"Label\"].str.startswith(\"DoS\")\n",
    "df.loc[mask, \"Label\"] = \"DoS\"\n",
    "\n",
    "# Group DDoS attacks\n",
    "mask = df[\"Label\"].str.lower().str.startswith(\"ddos\")\n",
    "df.loc[mask, \"Label\"] = \"DDoS\"\n",
    "\n",
    "# Group Web attacks\n",
    "mask = df[\"Label\"].str.startswith(\"Web Attack\")\n",
    "df.loc[mask, \"Label\"] = \"Web Attack\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Updated class imbalance\n",
    "mask = df[\"Label\"] != normal_label\n",
    "mod_ad_ratios = pd.DataFrame(\n",
    "    pd.concat(\n",
    "        (df[mask][\"Label\"].value_counts(),\n",
    "        df[mask][\"Label\"].value_counts() / len(df[mask])), axis=1),\n",
    ")\n",
    "mod_ad_ratios.columns = [\"Count\", \"Ratio\"]\n",
    "mod_ad_ratios"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check unique values\n",
    "Drop columns with unique values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uniq_cols = df.columns[df.nunique() <= 1].tolist()\n",
    "stats[\"n_unique_cols\"] = len(uniq_cols)\n",
    "if uniq_cols:\n",
    "    print(\"Found {} columns with unique values: {}\".format(len(uniq_cols), uniq_cols))\n",
    "    stats[\"unique_cols\"] = \", \".join([str(col) for col in uniq_cols])\n",
    "    df.drop(uniq_cols, axis=1, inplace=True)\n",
    "    stats[\"n_dropped_cols\"] += len(uniq_cols)\n",
    "    uniq_cols = df.columns[df.nunique() <= 1].tolist()\n",
    "assert len(uniq_cols) == 0, \"Found {} columns with unique values: {}\".format(len(uniq_cols), uniq_cols)\n",
    "print(\"Columns are valid with more than one distinct value\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check for NaN/invalid values\n",
    "First, find the columns with NaN values. Further processing will be required if we find any."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#n_dropped = len(df[df[\"Flow Duration\"].isna()])\n",
    "#df = df[df[\"Flow Duration\"].isna()].dropna()\n",
    "n_dropped = len(df[df[\"Flow Duration\"].isna()])\n",
    "stats[\"n_dropped_rows\"] += n_dropped\n",
    "df = df.drop(index=df[df[\"Flow Duration\"].isna()].index)\n",
    "print(\"Dropped {} rows\".format(n_dropped))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replacing INF values with NaN\n",
    "df = df.replace([-np.inf, np.inf], np.nan)\n",
    "nan_cols = df.columns[df.isna().sum() > 0].tolist()\n",
    "stats[\"n_nan_cols\"] = len(nan_cols)\n",
    "if nan_cols:\n",
    "    stats[\"nan_cols\"] = \", \".join([str(col) for col in nan_cols])\n",
    "print(\"Found NaN columns: {}\".format(nan_cols))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having found two columns with NaN values, we must investigate further before taking any decision."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping them seems risky because we would also lose anomalies which are already scarce and important for evaluation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)][\"Label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping them seems risky since we also drop anomalies which are already scarce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check different values in Flow Duration\n",
    "print(df[df.isna().any(axis=1)][\"Flow Duration\"].unique())\n",
    "# Count number of nan instances when `Flow Duration` > 0\n",
    "df[df[\"Flow Duration\"] > 0].isna().sum().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`Flow Bytes/s` and `Flow Packets/s` must be computed from `Flow Duration`. When the latter columns is zero, a division by zero occurs and the first two columns have NaN values. Zero values in `Flow Duration` are probably due to a lack of precision in the data type used. They must be associated with flows that lasted nanoseconds. Hence, we can convert the NaN rows to zeros."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_dropped = df[nan_cols].isna().sum()[0]\n",
    "df = df.fillna(0)\n",
    "print(\"Replaced {} rows or {:2.4f}% of original data\".format(n_dropped, n_dropped / len(df)))\n",
    "remaining_nans = df.isna().sum().sum()\n",
    "assert remaining_nans == 0, \"There are still {} NaN values\".format(remaining_nans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check for negative values\n",
    "Most of the features should be strictly positive. For instance, a packet with a negative number of bytes makes no sense."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(exclude=\"object\").columns\n",
    "mask = (df[num_cols] < 0).sum() > 0\n",
    "neg_cols = df[num_cols].columns[mask]\n",
    "stats[\"n_negative_cols\"] = len(neg_cols)\n",
    "stats[\"negative_cols\"] = \", \".join(neg_cols)\n",
    "print(\"Found {} columns with negative values: {}\".format(len(neg_cols), neg_cols))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame(\n",
    "    pd.concat((\n",
    "        (df[neg_cols] < 0).sum(),\n",
    "        (df[neg_cols] < 0).sum() / len(df)\n",
    "    ), axis=1)\n",
    ")\n",
    "neg_df.columns = [\"Count\", \"Ratio\"]\n",
    "neg_df = neg_df.sort_values(\"Count\", ascending=False)\n",
    "neg_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print((df[\"Init Bwd Win Byts\"][df[\"Init Bwd Win Byts\"] < 0]).unique())\n",
    "print((df[\"Init Fwd Win Byts\"][df[\"Init Fwd Win Byts\"] < 0]).unique())\n",
    "#df[df[\"Init_Win_bytes_backward\"] < 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop `Init_Win_bytes_forward` and `Init_Win_bytes_backward` because too many of their values are equal to -1 which makes no sense.\n",
    "to_drop = neg_df[neg_df[\"Ratio\"] > 0.01].index.tolist()\n",
    "df = df.drop(to_drop, axis=1)\n",
    "neg_df = neg_df.drop(to_drop)\n",
    "stats[\"n_dropped_cols\"] += len(to_drop)\n",
    "stats[\"dropped_cols\"] = stats[\"dropped_cols\"] + \", \".join(to_drop)\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "print(\"Dropped {} columns: {}\".format(len(to_drop), to_drop))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[(df[num_cols] < 0).any(1)][\"Label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The remaining invalid values are associated only to 15 benign rows. Removing them is probably the safest solution here."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[(df[num_cols] < 0).any(1)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When Flow Duration < 0, multiple columns are negative. Since these rows are only associated with BENIGN flows, we can drop them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_dropped = (df[\"Flow Duration\"] < 0).sum()\n",
    "stats[\"n_dropped_rows\"] += n_dropped\n",
    "df = df[df[\"Flow Duration\"] >= 0]\n",
    "print(\"Dropped {} rows\".format(n_dropped))\n",
    "# assert len(df[(df[num_cols] < 0).any(1)]) == 0, \"there are still negative rows\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(index=df[(df[num_cols] < 0).any(1)].index)\n",
    "stats[\"n_dropped_rows\"] += 1\n",
    "assert len(df[(df[num_cols] < 0).any(1)]) == 0, \"there are still negative rows\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"Category\"] = df[\"Label\"]\n",
    "df[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == normal_label else 1)\n",
    "df[\"Label\"] = df[\"Label\"].astype(np.uint8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalize attributes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "assert np.allclose(df[num_cols].max(axis=0).to_numpy(), 1.), \"Found values different than 1.\"\n",
    "assert np.allclose(df[num_cols].min(axis=0).to_numpy(), 0.), \"Found values lesser than 0.\"\n",
    "print(\"Data is scaled between 0 and 1\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats[\"n_final_features\"] = df.shape[1] - 2\n",
    "stats[\"n_final_rows\"] = df.shape[0]\n",
    "stats[\"final_anomaly_ratio\"] = (df[\"Label\"] != 0).sum() / len(df)\n",
    "stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store processed dataset to CSV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv(export_path + \"/\" + export_fname, index=False)\n",
    "print(\"Processed data saved under: {}\".format(export_path + \"/\" + export_fname))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}